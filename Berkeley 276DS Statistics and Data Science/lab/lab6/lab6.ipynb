{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab6.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1><center>SDSE Lab 6 <br><br> Ensemble methods and hyperparameter tuning </center></h1>\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab exercise we will apply several classification models to a problem in astrophysics. The problem is described [here](https://satyam5120.medium.com/predicting-a-pulsar-star-using-different-machine-learning-algorithms-d22ee8fc71b4) and [here](https://www.kaggle.com/datasets/colearninglounge/predicting-pulsar-starintermediate). It consists in labeling observations of space objects as either pulsars or not pulsars, based on the properties of an 'integrated profile' and on the DM-SNR curve. The dataset has 8 feature columns:\n",
    "1. Mean of the integrated profile.\n",
    "2. Standard deviation of the integrated profile.\n",
    "3. Excess kurtosis of the integrated profile.\n",
    "4. Skewness of the integrated profile.\n",
    "5. Mean of the DM-SNR curve.\n",
    "6. Standard deviation of the DM-SNR curve.\n",
    "7. Excess kurtosis of the DM-SNR curve.\n",
    "8. Skewness of the DM-SNR curve.\n",
    "\n",
    "Our goal is to choose a classification model from amongst the ones we've learned in class. The procedure will follow these steps:\n",
    "1. Load the data.\n",
    "2. Remove null values.\n",
    "3. Find number of samples per class.\n",
    "4. Extract a test dataset.\n",
    "5. Build models:\n",
    "    + Logistic regression\n",
    "    + Random forest\n",
    "    + AdaBoost\n",
    "    + Gradient Boosted Trees\n",
    "6. Select and evaluate a final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T02:39:10.457118Z",
     "iopub.status.busy": "2023-11-14T02:39:10.456523Z",
     "iopub.status.idle": "2023-11-14T02:39:11.357013Z",
     "shell.execute_reply": "2023-11-14T02:39:11.356148Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lab6_utils as lab6\n",
    "rng_seed = 2434"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the data\n",
    "\n",
    "+ Load the data file `pulsar_data.csv` into a Pandas dataframe using [`pd.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)\n",
    "+ Save the column headers corresponding to feature names (all except the `target_class`) to the variable `feature_names` ([Hint](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.columns.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T02:39:11.361788Z",
     "iopub.status.busy": "2023-11-14T02:39:11.360916Z",
     "iopub.status.idle": "2023-11-14T02:39:11.389460Z",
     "shell.execute_reply": "2023-11-14T02:39:11.388701Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rawdata = ...\n",
    "feature_names = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Remove null values\n",
    "\n",
    "Remove any feature column of `rawdata` that has more than zero null values. To check for null values you can use the [`isnull`](https://pandas.pydata.org/docs/reference/api/pandas.isnull.html) method on each feature column. To remove a column, use the [`drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) method with the `columns` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T02:39:11.451494Z",
     "iopub.status.busy": "2023-11-14T02:39:11.451095Z",
     "iopub.status.idle": "2023-11-14T02:39:11.464188Z",
     "shell.execute_reply": "2023-11-14T02:39:11.463245Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for feature in feature_names:\n",
    "    if ...:\n",
    "        rawdata = rawdata.drop(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Number of samples per class\n",
    "Find the number of data points in each of the two classes. Save the number of class 0 samples and class 1 samples to `N0` and `N1` respectively. Notice that there is a significant imbalance of negative and positive samples in this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T02:39:11.482982Z",
     "iopub.status.busy": "2023-11-14T02:39:11.482501Z",
     "iopub.status.idle": "2023-11-14T02:39:11.491218Z",
     "shell.execute_reply": "2023-11-14T02:39:11.490311Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N0 = ...\n",
    "N1 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extract the test dataset\n",
    "\n",
    "Use scikit-learn's `train_test_split` method to split `rawdata` into trainging and testing parts. Keep 90% for training and 10% for testing. Remember to pass `random_state=rng_seed` to `train_test_split` so that the result is repeatable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T02:39:11.516374Z",
     "iopub.status.busy": "2023-11-14T02:39:11.515958Z",
     "iopub.status.idle": "2023-11-14T02:39:12.015902Z",
     "shell.execute_reply": "2023-11-14T02:39:12.015045Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Logistic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Training\n",
    "\n",
    "In this part we will use grid search to choose the values of the hyperparameters of the logistic regression pipeline. Begin by creating a pipeline with a `StandardScaler` followed by `LogisticRegression` classifier. Pass these parameters to the contructor of the logistic regression classifier:\n",
    "+ `solver`: `liblinear`\n",
    "+ `random_state`: `rng_seed`\n",
    "\n",
    "Then fit the model using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T02:39:12.051516Z",
     "iopub.status.busy": "2023-11-14T02:39:12.050895Z",
     "iopub.status.idle": "2023-11-14T02:39:12.173178Z",
     "shell.execute_reply": "2023-11-14T02:39:12.172038Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model = Pipeline([\n",
    "    ('scaler' , ... ) ,\n",
    "    ('model' , ... )\n",
    "])\n",
    "model.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5p1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Hyperparameter tuning with grid search\n",
    "\n",
    "Run grid search (see documentation here: [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)) with 3-fold cross validation. The parameters to search over are:\n",
    "\n",
    "+ the regularization function `penalty`. Candidate values are `l1` and `l2`\n",
    "+ the regularization weight `C`. Candidate values are `np.logspace(-3, 1, 10)`.\n",
    "\n",
    "**Note**: For `GridSearchCV` to work with the pipeline, it needs to set the parameters of the model using their string names. Within the pipeline, the parameters of the model are prefixed with `model__`. For example, in this case we have \n",
    "\n",
    "```python\n",
    "param_grid = {\n",
    "    'model__penalty' : ['l1','l2'],\n",
    "    'model__C' : np.logspace(-3, 1, 10) }\n",
    "```\n",
    "\n",
    "Pass the following parameters to the `GridSearchCV` contructor.\n",
    "+ `param_grid=param_grid`\n",
    "+ `scoring=['accuracy','balanced_accuracy']`,\n",
    "+ `cv=cvfolds`,\n",
    "+ `refit='balanced_accuracy'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T02:39:12.218835Z",
     "iopub.status.busy": "2023-11-14T02:39:12.218489Z",
     "iopub.status.idle": "2023-11-14T02:39:15.225008Z",
     "shell.execute_reply": "2023-11-14T02:39:15.223422Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "cvfolds = 3\n",
    "param_grid = ...\n",
    "gs = GridSearchCV(...)\n",
    "gs = gs.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`lab6_utils.py` contains useful functions for unpacking and plotting the result of the grid search. Use the `lab6.unpack_gridsearch` method to extract information from the grid search solution. This method returns:\n",
    "1. A pandas dataframe with the cross-validated performance metrics for each point on the grid.\n",
    "2. A dictionary with the best-case hyperparameter values.\n",
    "3. The classifier corresponding to the best hyperparameter values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T02:39:15.230146Z",
     "iopub.status.busy": "2023-11-14T02:39:15.229455Z",
     "iopub.status.idle": "2023-11-14T02:39:15.254614Z",
     "shell.execute_reply": "2023-11-14T02:39:15.253044Z"
    }
   },
   "outputs": [],
   "source": [
    "result_logreg, best_params_logreg, best_model_logreg, best_score_logreg = lab6.unpack_gridsearch(gs,param_grid,cvfolds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lab6.plot_grid_result` plots the results of the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T02:39:15.259125Z",
     "iopub.status.busy": "2023-11-14T02:39:15.258408Z",
     "iopub.status.idle": "2023-11-14T02:39:15.807414Z",
     "shell.execute_reply": "2023-11-14T02:39:15.806701Z"
    }
   },
   "outputs": [],
   "source": [
    "lab6.plot_grid_result(result_logreg,param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5p2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Package it into a function\n",
    "\n",
    "Next we will repeat this process with several other classification models. To keep the code clean, we will first collect the steps into a single function. This function receives as inputs:\n",
    "1. The classifier object, e.g. `LogisticRegression(solver='liblinear',random_state=rng_seed)`\n",
    "2. The `param_grid` dictionary that defines the search space for `GridSearchCV`.\n",
    "\n",
    "It should:\n",
    "1. Create the pipeline model.\n",
    "2. Construct the `GridSearchCV` object as was done in part 5.2\n",
    "3. Run `fit` on the grid search object, using the training data (no need to pass the data as an argument to `build_grid_plot`; it's a global variable)\n",
    "4. Run `unpack_gridsearch` to obtain `result`, `best_params`, `best_model`, and `best_score`.\n",
    "5. Plot the result with `lab6.plot_grid_result`\n",
    "6. `return result, best_params, best_model, best_score`\n",
    "\n",
    "\n",
    "**Note**: This part has no autograder test. Any errors in the `build_grid_plot` function should be caught by tests in subsequent parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T02:39:15.850329Z",
     "iopub.status.busy": "2023-11-14T02:39:15.850169Z",
     "iopub.status.idle": "2023-11-14T02:39:15.855070Z",
     "shell.execute_reply": "2023-11-14T02:39:15.854667Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_grid_plot(model,param_grid):\n",
    "    \n",
    "    global Xtrain, ytrain, cvfolds\n",
    "\n",
    "    # 1. Create the pipeline model\n",
    "    model = Pipeline([\n",
    "    ...\n",
    "    ...\n",
    "    ])\n",
    "\n",
    "    # 2. Construct the `GridSearchCV` object as was done in part 5.1.2\n",
    "    gs = GridSearchCV(...)\n",
    "    \n",
    "    # 3. Run `fit` on the grid search object, using the training data (no need to pass the data as an argument to `build_grid_plot`; it's a global variable)\n",
    "    gs = gs.fit(...)\n",
    "    \n",
    "    # 4. Run `unpack_gridsearch` to obtain `result`, `best_params`, `best_model`, and `best_score`.\n",
    "    result, best_params, best_model, best_score = lab6.unpack_gridsearch(...)\n",
    "\n",
    "    # 5. Plot the result with `lab6.plot_grid_result`\n",
    "    lab6.plot_grid_result(...)\n",
    "\n",
    "    # 6. `return result, best_params, best_model, best_score`\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Random forest\n",
    "\n",
    "Use the following parameters for the hyper-parameter search:\n",
    "+ `max_features: ['sqrt','log2']`\n",
    "+ `n_estimators: np.linspace(2, 100, 5, dtype=int)`\n",
    "\n",
    "**Note**: Remember to set the random state for the model in the model's constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T02:39:15.857279Z",
     "iopub.status.busy": "2023-11-14T02:39:15.857107Z",
     "iopub.status.idle": "2023-11-14T02:39:39.761322Z",
     "shell.execute_reply": "2023-11-14T02:39:39.760840Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid_rf = ...\n",
    "\n",
    "result_rf, best_params_rf, best_model_rf, best_score_rf = build_grid_plot(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. AdaBoost\n",
    "\n",
    "Use the following parameters for the hyper-parameter search:\n",
    "+ `learning_rate: [0.01,0.1]`\n",
    "+ `n_estimators: np.linspace(20, 100, 5, dtype=int)`\n",
    "\n",
    "**Note**: Remember to set the random state for the model in the model's constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T02:39:39.791105Z",
     "iopub.status.busy": "2023-11-14T02:39:39.790756Z",
     "iopub.status.idle": "2023-11-14T02:39:55.359584Z",
     "shell.execute_reply": "2023-11-14T02:39:55.358868Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "param_grid_ab = ...\n",
    "\n",
    "result_ab, best_params_ab, best_model_ab, best_score_ab = build_grid_plot(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Gradient Boosted Trees\n",
    "\n",
    "Use the following parameters for the hyper-parameter search:\n",
    "+ `learning_rate: [0.1,1.0]`\n",
    "+ `n_estimators: [50,75,100]`\n",
    "\n",
    "**Note**: Remember to set the random state for the model in the model's constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T02:39:55.390677Z",
     "iopub.status.busy": "2023-11-14T02:39:55.390502Z",
     "iopub.status.idle": "2023-11-14T02:40:18.779180Z",
     "shell.execute_reply": "2023-11-14T02:40:18.778546Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "param_grid_gbc = ...\n",
    "\n",
    "result_gbc, best_params_gbc, best_model_gbc, best_score_gbc = build_grid_plot(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Final model selection\n",
    "\n",
    "We have now build 4 separate classifiers: logistic regression, random forest, AdaBoost, and Gradient boosted trees. These are stored below in the `all_models` list. Select from this ist the classifier with the best score. Save the corresponding name, model, and score to variables `best_name`, `best_model`, and `best_score` respoenctively. \n",
    "\n",
    "Then compute the test accuracy and balanced accuracy for the selected model, using its `predict` function and the `accuracy_score` and `balanced_accuracy_score` functions. Store these as `test_accuracy` and `test_balanced_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T02:40:18.820032Z",
     "iopub.status.busy": "2023-11-14T02:40:18.819676Z",
     "iopub.status.idle": "2023-11-14T02:40:18.832316Z",
     "shell.execute_reply": "2023-11-14T02:40:18.831679Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "\n",
    "all_models = [\n",
    "    ('logreg',best_model_logreg, best_score_logreg),\n",
    "    ('rf',best_model_rf, best_score_rf),\n",
    "    ('ab',best_model_ab, best_score_ab),\n",
    "    ('gbc',best_model_gbc, best_score_gbc),\n",
    "]\n",
    "\n",
    "...\n",
    "best_name = ...\n",
    "best_model = ...\n",
    "best_score = ...\n",
    "\n",
    "ypred = ...\n",
    "test_accuracy = ...\n",
    "test_balanced_accuracy = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> set(feature_names)=={' Mean of the integrated profile',\n...        ' Standard deviation of the integrated profile',\n...        ' Excess kurtosis of the integrated profile',\n...        ' Skewness of the integrated profile', ' Mean of the DM-SNR curve',\n...        ' Standard deviation of the DM-SNR curve',\n...        ' Excess kurtosis of the DM-SNR curve',\n...        ' Skewness of the DM-SNR curve'}\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> set(rawdata.columns)=={' Excess kurtosis of the DM-SNR curve',\n...  ' Mean of the DM-SNR curve',\n...  ' Mean of the integrated profile',\n...  ' Skewness of the integrated profile',\n...  ' Standard deviation of the integrated profile', 'target_class'}\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> N0==11375\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> N1==1153\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> Xtrain.shape==(11275, 5) and Xtest.shape==(1253, 5)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.allclose(Xtrain.iloc[:5,:5].values,np.array([[114.0625,49.06544577,   0.26177655,   5.4506689 ,5.92887825],[129.015625  ,  51.37864713,  -0.39287668,   4.06688963,6.99992854],[118.890625  ,  52.01971297,  -0.3249598 ,   3.32107023,6.89619648],[130.109375  ,  50.53699672,  -0.1963287 ,   2.95986622,8.40783101],[138.8125    ,  47.61101337,  -0.15766978,   2.55267558,9.81723671]]),1e-2)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.allclose(Xtest.iloc[:5,:5].values,np.array([[ 1.00710938e+02,  3.95873544e+01,  1.90577125e+00, 3.03595318e+00,  7.74167135e+00],[ 1.13593750e+02,  5.08120676e+01,  1.06782387e-01, 2.64464883e+00,  8.98869263e+00],[ 7.64843750e+01,  4.03835100e+01,  1.83528897e+00, 2.72157191e+00,  8.33382076e+00],[ 1.16882812e+02,  5.28183238e+01, -3.92660094e-01, 7.85033445e+00,  4.32790715e+00],[ 1.10078125e+02,  5.08617319e+01, -3.52484187e-01, 1.71739130e+00,  1.11219638e+01]]),1e-2)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5p1": {
     "name": "q5p1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.allclose(np.sort(model.named_steps['scaler'].scale_),[ 4.52959342,  6.18703268,  6.77911222, 25.66842747, 29.56810699],1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.allclose(np.sort(model.named_steps['scaler'].mean_),[1.77042614,   8.33728113,  12.65272831,  46.52346098,111.00540743] ,1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5p2": {
     "name": "q5p2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> best_params_logreg['model__penalty']=='l2' and np.isclose(best_params_logreg['model__C'],1.2915496650148826,1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.allclose(np.sort(result_logreg['l1']['mean_test_accuracy'].values),[0.93073139, 0.95716184, 0.96691783, 0.97082026, 0.97206199,0.97321497, 0.9745453 , 0.9744566 , 0.97454527, 0.97454527],1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.allclose(np.sort(result_logreg['l1']['mean_test_balanced_accuracy'].values),[0.6219554 , 0.76969708, 0.8298979 , 0.8559829 , 0.86580857,0.87253708, 0.87979469, 0.87974587, 0.88022896, 0.88022896],1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.allclose(np.sort(result_logreg['l2']['mean_test_accuracy'].values),[0.96798216, 0.96833691, 0.97028809, 0.97135244, 0.97179591,0.97312627, 0.9740131 , 0.97454527, 0.97454527, 0.97463397],1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.allclose(np.sort(result_logreg['l2']['mean_test_balanced_accuracy'].values),[0.84155829, 0.84397568, 0.85220597, 0.8584528 , 0.86392081,0.87118123, 0.87688771, 0.88022896, 0.88022896, 0.88027778],1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.allclose(np.sort(result_rf['sqrt']['mean_test_accuracy'].values),[0.96993343, 0.97516615, 0.97578697, 0.9764078 , 0.97649655],1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.allclose(np.sort(result_rf['sqrt']['mean_test_balanced_accuracy'].values),[0.860297  , 0.89275702, 0.89745413, 0.8991489 , 0.89996863],1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.allclose(np.sort(result_rf['log2']['mean_test_accuracy'].values),[0.96993343, 0.97516615, 0.97578697, 0.9764078 , 0.97649655],1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.allclose(np.sort(result_rf['log2']['mean_test_balanced_accuracy'].values),[0.860297  , 0.89275702, 0.89745413, 0.8991489 , 0.89996863],1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> best_params_rf=={'model__max_features': 'sqrt', 'model__n_estimators': 75}\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.allclose(np.sort(result_ab[0.01]['mean_test_accuracy'].values),[0.96869173, 0.96878043, 0.96931251, 0.96993332, 0.97108625],1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.allclose(np.sort(result_ab[0.01]['mean_test_balanced_accuracy'].values),[0.85403613, 0.85917348, 0.85926603, 0.85959107, 0.86056515],1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.allclose(np.sort(result_ab[0.1]['mean_test_accuracy'].values),[0.97161842, 0.97223932, 0.97268272, 0.97303754, 0.97365839],1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.allclose(np.sort(result_ab[0.1]['mean_test_balanced_accuracy'].values),[0.85966167, 0.86430433, 0.86750053, 0.87074414, 0.87321404],1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> best_params_ab=={'model__learning_rate': 0.1, 'model__n_estimators': 100}\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8": {
     "name": "q8",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.allclose(np.sort(result_gbc[0.1]['mean_test_accuracy'].values),[0.97525489, 0.97525494, 0.97569827],1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.allclose(np.sort(result_gbc[0.1]['mean_test_balanced_accuracy'].values),[0.89861049, 0.89972225, 0.90102928],1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.allclose(np.sort(result_gbc[1.0]['mean_test_accuracy'].values),[0.93002323, 0.93020063, 0.93091028],1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.allclose(np.sort(result_gbc[1.0]['mean_test_balanced_accuracy'].values),[0.66417237, 0.66510045, 0.66649721],1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> best_params_gbc=={'model__learning_rate': 0.1, 'model__n_estimators': 100}\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9": {
     "name": "q9",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> best_name=='gbc'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.isclose(test_accuracy,0.9736632083000798,1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.isclose(test_balanced_accuracy,0.8885775228008237,1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
